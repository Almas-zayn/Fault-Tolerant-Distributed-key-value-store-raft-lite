# Fault-Tolerant Distributed Key-Value Store â€“ Raft Lite

A lightweight implementation of a fault-tolerant distributed key-value store built using the Raft consensus algorithm.


## ğŸ”· What is Raft?

Raft is a **distributed consensus algorithm** used to maintain a replicated log across multiple servers. It ensures that a cluster of machines agrees on the same sequence of operations, even if some machines fail.

### Key Concepts:

* **Leader Election** â€“ A single leader manages the cluster; followers replicate the leaderâ€™s log.
* **Log Replication** â€“ Client commands are appended to the leader log and replicated to followers.
* **Fault Tolerance** â€“ System continues even if some nodes crash, as long as a majority stay alive.
* **Ease of Understanding** â€“ Raft is designed to be simpler to understand compared to Paxos.

---

## ğŸ”· Project Overview

This project implements a distributed key-value store backed by the Raft protocol.

### Features:

* Cluster-based key-value storage
* Leader election and heartbeat mechanism
* Log replication using Raft RPCs
* Write-Ahead Logging (WAL) for persistence
* Simple CLI client to interact with the cluster

The project is educational, helping understand how distributed consensus can be applied to build fault-tolerant systems.

---

## ğŸ“ Repository Structure

A clean breakdown of the project layout:

```
Fault-Tolerant-Distributed-key-value-store-raft-lite/
â”‚
â”œâ”€â”€ build/               # Build scripts, binaries, Makefile outputs
â”‚
â”œâ”€â”€ raft/                # Core Raft implementation
â”‚   â”œâ”€â”€ raft_node.c
â”‚   â”œâ”€â”€ raft_node.h
â”‚   â”œâ”€â”€ handle_server.c
â”‚   â”œâ”€â”€ replicate_entries.c
â”‚   â”œâ”€â”€ raft_helpers.c
â”‚   â”œâ”€â”€ raft_helpers.h
â”‚   â”œâ”€â”€ cmd_queue.h
â”‚   â”œâ”€â”€ cmd_queue.c
â”‚   â””â”€â”€ handle_raft.c
â”‚
â”œâ”€â”€ rpc/                 # RPC communication (AppendEntries, RequestVote)
â”‚   â”œâ”€â”€ network_socket.c
â”‚   â””â”€â”€ rpc.h
â”‚
â”œâ”€â”€ wal/                 # Write-Ahead Log persistence module
â”‚   â”œâ”€â”€ wal.c
â”‚   â””â”€â”€ wal.h
â”‚
â”œâ”€â”€ wal-logs/            # Actual WAL log files generated at runtime
â”‚
â”œâ”€â”€ ui/                  # Optional UI/monitoring utilities
â”‚   â”œâ”€â”€ ui.c
â”‚   â””â”€â”€ colors.h
â”‚
â”œâ”€â”€ kv_client           # CLI tool to interact with the distributed store
â”œâ”€â”€ kv_client.c 
â”‚
â”œâ”€â”€ postmaster.c         # Main server node entry point
â”œâ”€â”€ makefile             # Build script
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸš€ Getting Started

### **1. Clone the Repository**

```bash
git clone https://github.com/Almas-zayn/Fault-Tolerant-Distributed-key-value-store-raft-lite.git
cd Fault-Tolerant-Distributed-key-value-store-raft-lite
```

### **2. Build the Project**

```bash
make
```

### **3. Start Multiple Nodes**

Run these in separate terminals:

```bash
./build/postmaster 
```

### **4. Use the Key-Value Client**

```bash
./kv_client <port> 
```

---


If the leader fails, a follower will automatically take over.

---

## âš™ï¸ How It Works (Internal Flow)

### **1. Leader Election**

* Nodes start as followers.
* If no heartbeat received â†’ convert to candidate.
* Candidate requests votes.
* Majority votes â†’ becomes leader.

### **2. Log Replication**

* Leader receives commands from the client.
* Appends to its log.
* Sends *AppendEntries RPC* to followers.
* When majority stores â†’ commit.

### **3. Fault Tolerance**

* System works as long as majority nodes are alive.
* WAL ensures recovery after crashes.
* Logs always converge to a consistent state.


---

If you found this project useful, consider â­ starring the repository!
